from __future__ import annotations

import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
import torch
from torch.utils.data import Dataset

from coughcount.paths import ProjectPaths as P


def build_window_starts(
    duration_sec: float, window_sec: float, hop_sec: float
) -> list[float]:
    duration_sec = float(duration_sec)
    window_sec = float(window_sec)
    hop_sec = float(hop_sec)

    if duration_sec <= 0:
        return []
    if duration_sec <= window_sec:
        return [0.0]

    n = int(math.floor((duration_sec - window_sec) / hop_sec)) + 1
    starts = [i * hop_sec for i in range(n)]

    last = duration_sec - window_sec
    if last - starts[-1] > 1e-6:
        starts.append(last)
    return starts


@dataclass(frozen=True, slots=True)
class Sample:
    dir: Path
    S_path: Path
    t_path: Path
    density_path: Path
    meta: dict[str, Any]


@dataclass(frozen=True, slots=True)
class Window:
    si: int
    left: int
    right: int


class EdgeAIWindowDataset(Dataset):
    """
    Directory layout (generated by your npy script):
      <npy_dir>/
        out|body/
          <stem>/
            S.npy
            t.npy
            density.npy
            meta.json

    Returns sample:
      {
        "x": FloatTensor [F, T],
        "y": FloatTensor [T],
        "length": int,
        "count": float,
        "is_pos": int(0/1),
        "meta": dict (optional),
      }
    """

    def __init__(
        self,
        split: str,
        *,
        npy_dir: Path = P.edgeai_npy,
        mic: str = "both",  # "out" | "body" | "both"
        window_sec: float = 8.0,
        hop_sec: float = 4.0,
        pos_threshold: float = 0.01,
        return_meta: bool = True,
    ) -> None:
        self.npy_dir = Path(npy_dir)
        if not self.npy_dir.exists():
            raise FileNotFoundError(f"npy_dir not found: {self.npy_dir}")

        self.window_sec = float(window_sec)
        self.hop_sec = float(hop_sec)
        self.pos_threshold = float(pos_threshold)
        self.return_meta = bool(return_meta)

        self.subjects = self._load_split_subjects(split, P.edgeai_splits_json)

        self.samples = self._collect_samples(mic)
        if not self.samples:
            raise RuntimeError(f"No samples found in {self.npy_dir} (mic={mic})")

        self._data_cache: dict[int, tuple[np.ndarray, np.ndarray, np.ndarray]] = {}

        print(f"Loading {len(self.samples)} samples into memory...")
        for si, sp in enumerate(self.samples):
            S = np.load(sp.S_path).astype(np.float32)
            density = np.load(sp.density_path).astype(np.float32)
            t = np.load(sp.t_path)
            self._data_cache[si] = (S, density, t)
        
        total_mb = sum(
            s[0].nbytes + s[1].nbytes + s[2].nbytes 
            for s in self._data_cache.values()
        ) / (1024**2)
        print(f"Loaded {len(self._data_cache)} samples ({total_mb:.2f} MB)")

        self.windows: list[Window] = []
        self.pos_idx: list[int] = []
        self.neg_idx: list[int] = []
        self._index_windows()

        if not self.windows:
            raise RuntimeError("No windows after split filtering.")

    @staticmethod
    def _load_split_subjects(split: str, splits_path: Path) -> set[str]:
        if not splits_path.exists():
            raise FileNotFoundError(f"Splits not found: {splits_path}")

        with splits_path.open("r", encoding="utf-8") as f:
            splits = json.load(f)

        if split not in splits:
            raise KeyError(f"Split '{split}' not found in {splits_path}")

        return {str(x) for x in splits[split]}

    def _collect_samples(self, mic: str) -> list[Sample]:
        if mic == "both":
            mic_dirs = [self.npy_dir / "out", self.npy_dir / "body"]
        else:
            mic_dirs = [self.npy_dir / mic]

        samples: list[Sample] = []
        for mic_dir in mic_dirs:
            if not mic_dir.exists():
                continue

            for meta_path in sorted(mic_dir.glob("*/meta.json")):
                sample_dir = meta_path.parent
                S_path = sample_dir / "S.npy"
                t_path = sample_dir / "t.npy"
                density_path = sample_dir / "density.npy"
                if not (S_path.exists() and t_path.exists() and density_path.exists()):
                    continue

                with meta_path.open("r", encoding="utf-8") as f:
                    meta = json.load(f)

                subject_id = str(meta.get("subject_id", ""))
                if subject_id not in self.subjects:
                    continue

                samples.append(
                    Sample(
                        dir=sample_dir,
                        S_path=S_path,
                        t_path=t_path,
                        density_path=density_path,
                        meta=meta,
                    )
                )

        return samples

    def _get_sample_data(self, si: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """从缓存获取样本数据（已在 __init__ 中加载）"""
        return self._data_cache[si]

    def _index_windows(self) -> None:
        for si, sp in enumerate(self.samples):
            _, density, t = self._get_sample_data(si)
            
            if t.shape[0] == 0:
                continue

            t0 = float(t[0])
            duration = float(t[-1]) - t0
            starts = build_window_starts(duration, self.window_sec, self.hop_sec)
            if not starts:
                continue

            for s in starts:
                start_sec = t0 + float(s)
                end_sec = start_sec + self.window_sec

                left = int(np.searchsorted(t, start_sec, side="left"))
                right = int(np.searchsorted(t, end_sec, side="right"))
                if right <= left:
                    continue

                self.windows.append(Window(si=si, left=left, right=right))
                w_idx = len(self.windows) - 1

                window_sum = float(np.asarray(density[left:right]).sum())
                if window_sum > self.pos_threshold:
                    self.pos_idx.append(w_idx)
                else:
                    self.neg_idx.append(w_idx)

    def __len__(self) -> int:
        return len(self.windows)

    def __getitem__(self, idx: int) -> dict[str, Any]:
        w = self.windows[int(idx)]
        sp = self.samples[w.si]

        S, density, _ = self._get_sample_data(w.si)

        # 直接切片（数据已在内存中）
        S_win = S[:, w.left : w.right]
        y_win = density[w.left : w.right]

        count = float(y_win.sum())
        is_pos = 1 if count > self.pos_threshold else 0

        sample: dict[str, Any] = {
            "x": torch.from_numpy(S_win).to(torch.float32),
            "y": torch.from_numpy(y_win).to(torch.float32),
            "length": int(S_win.shape[1]),
            "count": count,
            "is_pos": is_pos,
        }

        if self.return_meta:
            meta = dict(sp.meta)
            meta["start_frame"] = int(w.left)
            meta["end_frame"] = int(w.right)
            sample["meta"] = meta

        return sample


def pad_collate(batch: list[dict[str, Any]]) -> dict[str, Any]:
    if not batch:
        z = torch.zeros(0)
        return {"x": z, "y": z, "lengths": z}

    xs = [b["x"] for b in batch]
    ys = [b["y"] for b in batch]

    lengths = torch.tensor([int(x.shape[-1]) for x in xs], dtype=torch.long)
    max_len = int(lengths.max().item())
    freq = int(xs[0].shape[0])

    x_pad = torch.zeros((len(xs), freq, max_len), dtype=xs[0].dtype)
    y_pad = torch.zeros((len(xs), max_len), dtype=torch.float32)

    for i, (x, y) in enumerate(zip(xs, ys)):
        t = int(x.shape[-1])
        x_pad[i, :, :t] = x
        y_pad[i, : int(y.shape[0])] = y

    out: dict[str, Any] = {
        "x": x_pad,
        "y": y_pad,
        "lengths": lengths,
        "count": torch.tensor([float(b["count"]) for b in batch], dtype=torch.float32),
        "is_pos": torch.tensor([int(b["is_pos"]) for b in batch], dtype=torch.long),
    }
    if "meta" in batch[0]:
        out["meta"] = [b["meta"] for b in batch]
    return out
